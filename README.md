# Compare LLMâ€™s

A fullâ€‘stack â€œCompare LLMâ€™sâ€ project that lets you send the same prompt to multiple largeâ€‘languageâ€‘model APIs (OpenAI, Anthropic, X.ai), see their outputs sideâ€‘byâ€‘side, track usage & cost, view timeâ€‘series analytics, and browse your request history.

---

## ğŸš€ Features

- **Backend (Node.js + Express)**
  - **`POST /generate`**  
    Send a prompt + model list + temperature + max_tokens â†’ fanâ€‘out concurrent calls to OpenAI, Anthropic, X.ai â†’ return each modelâ€™s response text, usage, cost, duration, into a single JSON envelope.
  - **`GET /requests`** (paged)  
    List past requests (UUID, timestamp, prompt, models, parameters).
  - **`GET /request/:id`**  
    Fetch one historic requestâ€™s full results.
  - **`GET /timeseries`**  
    Returns timeâ€‘series of perâ€‘model cost & latency (breakdown by request timestamp) for visualization.
  - **Prisma + MongoDB Atlas**  
    Persist every APIâ€‘call record and its usage/cost/latency.  

- **Frontend (React + Tailwind + Recharts)**
  - **TopBar** with logo & app title.
  - **LeftBar**  
    - Resizable history panel  
    - Paginated list of past requests  
    - Refresh button  
    - Click â†’ loads that request into main view
  - **ControlPanel**  
    - Model checkboxes (`open_ai`, `anthropic`, `x_ai`)  
    - Temperature slider (0â€“1)  
    - Maxâ€‘tokens input  
  - **PromptArea**  
    - Prompt textarea & â€œGenerateâ€ button  
    - Shows individual loaders while awaiting each model  
  - **PromptResponse**  
    - Sideâ€‘byâ€‘side panels rendering each modelâ€™s Markdown output  
    - Synchronized scrolling
  - **RightBar (Analytics)**  
    - **Timeâ€‘series** lineâ€‘charts: cost & latency over time  
    - **Barâ€‘charts**: cost & latency comparison for current request  
    - **Cards**: detailed usage / cost / latency per model  

---

## ğŸ“¦ Prerequisites

- [Docker](https://www.docker.com/) installed & running  
- A MongoDB Atlas cluster (or any MongoDB URI you prefer)  
- API keys for:
  - OpenAI
  - Anthropic
  - X.ai (X.ai / Grok)

---

## ğŸ”§ Setup & Run

1. **Clone this repo**  
   ```bash
   git clone <yourâ€‘repoâ€‘url>
   cd ai-comp
   ```
2. **Create .env for the backend**  
   in the `backend/.env` add:
   ```env
    # MongoDB Atlas connection URL (include database name)
    DATABASE_URL="mongodb+srv://<user>:<password>@cluster0.xxxx.mongodb.net/mydb?retryWrites=true&w=majority"

    # Express port (optional, defaults to 3000)
    PORT=3000

    # Your LLM API keys
    XAI_API_KEY=<your_xai_key>
    ANTHROPIC_API_KEY=<your_anthropic_key>
    OPENAI_API_KEY=<your_openai_key>

3. **Run everything with Docker Compose**  
   ```bash
   docker-compose up --build
   ```
   - Backend available at http://localhost:3000
   - Prisma Studio at http://localhost:5555
   - Frontend at http://localhost:3001

4. **Browse & compare**
   - Navigate to http://localhost:3001
   - Enter a prompt, select models, click Generate
   - View sideâ€‘byâ€‘side outputs, usage, cost, latency
   - Explore history in the left panel, analytics in the right panel

## ğŸ“‚ Project Structure
```txt
ai-comp/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ prisma/
â”‚   â”‚   â””â”€â”€ schema.prisma
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â””â”€â”€ .env
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ api/
â”‚       â”œâ”€â”€ components/
â”‚       â”œâ”€â”€ context/
â”‚       â”œâ”€â”€ App.js
â”‚       â””â”€â”€ index.js
â””â”€â”€ docker-compose.yml
```